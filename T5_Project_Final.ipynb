{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "importing needed libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "import sys\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy\n",
    "\n",
    "import pyodbc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of dates for the last 12 weeks\n",
    "\n",
    "files_to_read = [(datetime.strptime(\"2021-09-18\",'%Y-%m-%d') - timedelta(days=i*7)).strftime('%y%m%d') for i in range(0,12)]\n",
    "\n",
    "dfs =[]\n",
    "for f in tqdm(files_to_read):\n",
    "    file_url = f\n",
    "    #Local\n",
    "    dfs.append(pd.read_csv(\"D:\\\\proj\\\\1\\\\turnstile_\"+str(f)+\".txt\"))\n",
    "    #Remote\n",
    "    #dfs.append(pd.read_csv(\"http://web.mta.info/developers/data/nyct/turnstile/turnstile_\"+str(f)+\".txt\"))\n",
    "dfs2 = pd.concat(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load to database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading using for loop \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "('01000', '[01000] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionRead (recv()). (65534) (SQLEndTran); [01000] [Microsoft][ODBC SQL Server Driver][DBNETLIB]General network error. Check your network documentation. (11)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-3d82520d1811>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m             cursor.execute(\"Insert into MTA_raw (C_A,Unit,SCP,Station,LineName,Division,Date,Time,Descr,Entries,Exits) \\\n\u001b[0;32m     30\u001b[0m             values (?,?,?,?,?,?,?,?,?,?,?)\",(row['C/A'],row['UNIT'],row['SCP'],row['STATION'],row['LINENAME'],row['DIVISION'],row['DATE'],row['TIME'],row['DESC'],row['ENTRIES'],row['EXITS']))\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mcnxn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mi\u001b[0m  \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mError\u001b[0m: ('01000', '[01000] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionRead (recv()). (65534) (SQLEndTran); [01000] [Microsoft][ODBC SQL Server Driver][DBNETLIB]General network error. Check your network documentation. (11)')"
     ]
    }
   ],
   "source": [
    "# Local db info \n",
    "#server = 'LAPTOP-88OFGUMN\\SQLEXPRESS' \n",
    "#database = 'T5_Proj1' \n",
    "#username = 'sa' \n",
    "#password = '102030' \n",
    "\n",
    "## cloud db info\n",
    "server = 't5.database.windows.net' \n",
    "database = 'T5' \n",
    "username = 't5' \n",
    "password = 'My404Data' \n",
    "dfs2.columns=dfs2.columns.str.strip()\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "#cols = ['C_A','UNIT','SCP','STATION','LINENAME','DIVISION','DATE','TIME','DESCR','ENTRIES','EXITS','DateTime']\n",
    "#dfs2.columns = cols\n",
    "\n",
    "#dfs2['EXITS'] = pd.to_numeric(dfs2['EXITS'])\n",
    "#dfs2['ENTRIES'] = pd.to_numeric(dfs2['ENTRIES'])\n",
    "i =0\n",
    "\n",
    "# delete rows before insert, to prevent duplicates \n",
    "cursor.execute('delete from mta_raw')\n",
    "cnxn.commit()\n",
    "\n",
    "for index, row in dfs2.iterrows():\n",
    "            #print(row)\n",
    "            cursor.execute(\"Insert into MTA_raw (C_A,Unit,SCP,Station,LineName,Division,Date,Time,Descr,Entries,Exits) \\\n",
    "            values (?,?,?,?,?,?,?,?,?,?,?)\",(row['C/A'],row['UNIT'],row['SCP'],row['STATION'],row['LINENAME'],row['DIVISION'],row['DATE'],row['TIME'],row['DESC'],row['ENTRIES'],row['EXITS']))\n",
    "            cnxn.commit()\n",
    "            i  +=1\n",
    "\n",
    "\n",
    "        \n",
    "cols =  ['C/A','UNIT','SCP','STATION','LINENAME','DIVISION','DATE','TIME','DESC','ENTRIES','EXITS','DateTime']\n",
    "dfs2.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Local db info\n",
    "#server = 'LAPTOP-88OFGUMN\\SQLEXPRESS' \n",
    "#database = 'T5_Proj1' \n",
    "#username = 'sa' \n",
    "#password = '102030' \n",
    "#cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "#cursor = cnxn.cursor()\n",
    "\n",
    "## cloud db info\n",
    "server = 't5.database.windows.net' \n",
    "database = 'T5' \n",
    "username = 't5' \n",
    "password = 'My404Data' \n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = cnxn.cursor()\n",
    "dfs2.head()\n",
    "#rename columns to match db \n",
    "cols = ['C_A','UNIT','SCP','STATION','LINENAME','DIVISION','DATE','TIME','DESCR','ENTRIES','EXITS','DateTime']\n",
    "dfs2.columns = cols\n",
    "\n",
    "# delete rows before insert, to prevent duplicates \n",
    "cursor.execute('delete from mta_raw2')\n",
    "cnxn.commit()\n",
    "\n",
    "\n",
    "engine = sqlalchemy.create_engine(\"mssql+pyodbc://t5:My404Data@t5.database.windows.net/t5?driver=ODBC+Driver+17+for+SQL+Server\")\n",
    "\n",
    "\n",
    "dfs2.to_sql(\"mta_raw2\",engine,index=False,method='multi',if_exists='append',chunksize=100)\n",
    "\n",
    "\n",
    "#retract changes \n",
    "cols =  ['C/A','UNIT','SCP','STATION','LINENAME','DIVISION','DATE','TIME','DESC','ENTRIES','EXITS','DateTime']\n",
    "dfs2.columns = cols\n",
    "dfs2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data columns\n",
    "add column as Turnstile ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse Date + Time in one column \n",
    "dfs2[\"DateTime\"] =  pd.to_datetime(dfs2['DATE'] + ' ' + dfs2['TIME'])\n",
    "#Unique turnstile ID\n",
    "dfs2['Turnstile_ID'] = dfs2['C/A'] + '-' + dfs2['UNIT'] + '-' + dfs2['SCP']\n",
    "# add hour and minutes as columns\n",
    "dfs2[\"Hours\"] =dfs2.DateTime.dt.hour\n",
    "dfs2[\"Minutes\"] =  dfs2.DateTime.dt.minute\n",
    "\n",
    "rows_Hour = dfs2.groupby(\"Hours\").size()\n",
    "rows_Hour.columns = ['Count']\n",
    "# pivot the dataframe from long to wide form\n",
    "#result = rows_Hour.pivot(index='Hour', columns='Hour', values='Count')\n",
    "#print(result)\n",
    "ax = sns.heatmap(rows_Hour.values.reshape((24,1)),annot=True, fmt=\"d\")\n",
    "## add column with block datehour value for any record in between, and correct Hours, and Minutes\n",
    "## this value will be used to group any additional readings between timeblocks\n",
    "# group all values \n",
    "dfs2[\"DateHour\"] =0\n",
    "for i in range(0,6):\n",
    "    hr = i*4\n",
    "    if hr<10:\n",
    "        leadingZero = \"0\"+str(hr)\n",
    "    else:\n",
    "        leadingZero = str(hr)\n",
    "    print(str(hr),str(hr+4))\n",
    "    dfs2[\"DateHour\"] = np.where((dfs2['Hours']>=hr) & (dfs2['Hours']<(hr+4)),dfs2['DateTime'].dt.strftime('%Y%m%d'+leadingZero),dfs2[\"DateHour\"])\n",
    "    dfs2[\"Minutes\"] = np.where((dfs2['Hours']>=hr) & (dfs2['Hours']<(hr+4)),0,dfs2[\"Minutes\"])\n",
    "    dfs2[\"Hours\"] = np.where((dfs2['Hours']>=hr) & (dfs2['Hours']<(hr+4)),hr,dfs2[\"Hours\"])\n",
    "    \n",
    "\n",
    "    \n",
    "## removing duplicate rows by turnstile and datetime\n",
    "\n",
    "dfs2 = dfs2.sort_values(['Turnstile_ID', 'DateTime'])\n",
    "dfs2.columns=dfs2.columns.str.strip()\n",
    "print(dfs2.shape[0])\n",
    "\n",
    "\n",
    "#Check number of duplicate records duplicates\n",
    "print(\"duplicates :\",dfs2[dfs2.duplicated(['Turnstile_ID','DateTime'])].sort_values(['Turnstile_ID', 'DateTime']).shape[0])\n",
    "\n",
    "## drop duplicates \n",
    "dfs2.drop_duplicates(['Turnstile_ID','DateTime'],inplace=True)\n",
    "print(dfs2.shape[0])\n",
    "\n",
    "\n",
    "## calculate number of entries and exits\n",
    "dfs2['Num_Entries'] = np.where(dfs2.Turnstile_ID == dfs2.Turnstile_ID.shift(1), dfs2.ENTRIES.diff(), 0)\n",
    "dfs2['Num_Exits'] = np.where(dfs2.Turnstile_ID == dfs2.Turnstile_ID.shift(1), dfs2.EXITS.diff(), 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check percentage of missing values \n",
    "\n",
    "print(dfs2.columns)\n",
    "#for col in grp.columns:\n",
    " #   pct_missing = np.mean(grp[col].isna())\n",
    "#    print('{} - {}%'.format(col, round(pct_missing*100)))\n",
    "\n",
    "\n",
    "# check for negative results from previous calculation\n",
    "# find negative Entries \n",
    "print(\"Negative Entries\",dfs2[dfs2['Num_Entries']<0].shape[0])\n",
    "print(\"Negative Exits\",dfs2[dfs2['Num_Exits']<0].shape[0])\n",
    "\n",
    "## correct negative numbers\n",
    "\n",
    "\n",
    "#Large neg num_entries , and still large commulative Entries -> assume 0\n",
    "filt = (dfs2['Num_Entries']<-1000) & (dfs2['ENTRIES']>5000)\n",
    "dfs2.loc[filt,'Num_Entries'] = 0\n",
    "print(dfs2.loc[filt,'Num_Entries'].shape[0],\" rows set to 0 \")\n",
    "#Large neg num_entries , and small commulative Entries -> reset to 0 , take commulative as num_Entries\n",
    "filt = (dfs2['Num_Entries']<-1000) & (dfs2['ENTRIES']<=5000)\n",
    "dfs2.loc[filt,'Num_Entries'] = dfs2.loc[(dfs2['Num_Entries']<-1000) & (dfs2['ENTRIES']<=5000),'ENTRIES']\n",
    "print(dfs2.loc[filt,'Num_Entries'].shape[0],\" rows set to Entries\")\n",
    "# small neg num_entries, and large comulative Entries ->  take absolute value of the neg Num_Entries\n",
    "filt = (dfs2['Num_Entries']<0) & (dfs2['Num_Entries'] >=-1000) \n",
    "dfs2.loc[filt,'Num_Entries'] = abs(dfs2.loc[filt,'Num_Entries'])\n",
    "print(dfs2.loc[filt,'Num_Entries'].shape[0],\" rows set to absolute value\")\n",
    "\n",
    "\n",
    "##same for exits\n",
    "\n",
    "#Large neg num_entries , and still large commulative Entries -> assume 0\n",
    "filt = (dfs2['Num_Exits']<-1000) & (dfs2['EXITS']>5000)\n",
    "dfs2.loc[filt,'Num_Exits'] = 0\n",
    "print(dfs2.loc[filt,'Num_Exits'].shape[0],\"  exit rows set to 0 \")\n",
    "#Large neg num_entries , and small commulative Entries -> reset to 0 , take commulative as num_Entries\n",
    "filt = (dfs2['Num_Exits']<-1000) & (dfs2['EXITS']<=5000)\n",
    "dfs2.loc[filt,'Num_Exits'] = dfs2.loc[(dfs2['Num_Exits']<-1000) & (dfs2['EXITS']<=5000),'EXITS']\n",
    "print(dfs2.loc[filt,'Num_Exits'].shape[0],\" exit rows set to exits\")\n",
    "# small neg num_entries, and large comulative Entries ->  take absolute value of the neg Num_Entries\n",
    "filt = (dfs2['Num_Exits']<0) & (dfs2['Num_Exits'] >=-1000) \n",
    "dfs2.loc[filt,'Num_Exits'] = abs(dfs2.loc[filt,'Num_Entries'])\n",
    "print(dfs2.loc[filt,'Num_Exits'].shape[0],\" exit rows set to absolute value\")\n",
    "\n",
    "## Solving outliers\n",
    "\n",
    "## assuming no more than 5000 pass through turnstile in 4 hours block\n",
    "dfs2 = dfs2[dfs2['Num_Entries']<5000]\n",
    "dfs2 = dfs2[dfs2['Num_Exits']<5000]\n",
    "dfs2[(dfs2[\"Num_Entries\"]<0)|(dfs2[\"Num_Exits\"]<0)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring collection time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_Hour = dfs2.groupby(\"Hours\").size()\n",
    "rows_Hour.columns = ['Count']\n",
    "\n",
    "#after correction, all data points liy between 4 hours  starting from hour zero \n",
    "\n",
    "ax = sns.heatmap(rows_Hour.values.reshape((6,1)),annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if number of points is 6 per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by date and turnstil\n",
    "grp = dfs2.groupby([\"DATE\",\"Turnstile_ID\"]).size().reset_index()\n",
    "grp.columns = ['DATE','Turnstile_ID','Count']\n",
    "\n",
    "grp = grp[grp[\"Count\"]<6]\n",
    "print(grp['Count'].min(),' minimum readings for turnstile')\n",
    "print(grp['Count'].max(),' maximum reading for turnstile')\n",
    "print(grp['Count'].mean(),' avg readings for turnstile')\n",
    "\n",
    "\n",
    "Days_of_Irregular_readings = grp[grp['Count']>6].groupby('Turnstile_ID').size().reset_index()\n",
    "Days_of_Irregular_readings.columns = ['Turnstile_ID','Days']\n",
    "Days_of_Irregular_readings.boxplot('Days')\n",
    "#print(Days_of_Irregular_readings)\n",
    "\n",
    "grp.boxplot('Count')\n",
    "#using pivot to dra heatmap \n",
    "\n",
    "grp = grp.pivot(index='Turnstile_ID',columns='DATE',values='Count')\n",
    "fig, ax = plt.subplots(figsize=(20,10)) \n",
    "#mask = grp.isnull()\n",
    "ax = sns.heatmap(grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate distance between each record \n",
    "dfs2['DateHour'] =pd.to_numeric(dfs2['DateHour']) \n",
    "dfs2 = dfs2.sort_values(['Turnstile_ID','DATE', 'DateHour'])\n",
    "\n",
    "#print(dfs2.dtypes)\n",
    "dfs2[\"Dist\"] = np.where((dfs2.Turnstile_ID == dfs2.Turnstile_ID.shift(1)) & (dfs2.DATE == dfs2.DATE.shift(1)) ,dfs2.DateHour.diff(),0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  for turnstile-date that have between 2 to 6 records, we will interpolate the values and add records for missing values\n",
    "dfnorm = pd.DataFrame(columns=dfs2.columns)\n",
    "\n",
    "\n",
    "def interpolate(x):\n",
    "   \n",
    "    \n",
    "\n",
    "    #should check if the first record started after hour 00, and interpolate back to hour zero\n",
    "    # SOLVED - we did interpolate but forgot to change or remove the record that was interpolated\n",
    "    # turnstiles that have only one reading a day won't be processed because Dist will be 0.1 (default value for distinction)\n",
    "    #calc how many records to add\n",
    "    r = x.Dist // 4\n",
    "    calEntry = (x.ENTRIES / x.Dist) *4\n",
    "    calExit = (x.EXITS/x.Dist) *4\n",
    "\n",
    "    for i in range(int(r-1),0,-1):\n",
    "        ## loop\n",
    "\n",
    "        newtime = pd.datetime(x.DateTime.year, x.DateTime.month, x.DateTime.day, x.DateTime.hour, 0)\n",
    "        newtime = newtime - pd.Timedelta(4*i, unit='hours')\n",
    "        data = [x['C/A'],x.UNIT,x.SCP,x.STATION,x.LINENAME,x.DIVISION,x.DATE,x.TIME,'Norm',x.ENTRIES,x.EXITS,newtime,x.Turnstile_ID,calEntry,calExit,x.Hours-(4*i),x.Minutes,x.DateHour-(4*i),x.Dist]\n",
    "        dfnorm.loc[len(dfnorm)] = data\n",
    "    dfs2.loc[(dfs2['Turnstile_ID']==x.Turnstile_ID) & (dfs2['DateHour']==x.DateHour),'Num_Entries'] = calEntry\n",
    "def rfun(thevalue):\n",
    "    print(thevalue.shape)\n",
    "    return 0 \n",
    "\n",
    "dfs2[dfs2[\"Dist\"]>4].apply(lambda x:interpolate(x) ,axis=1)\n",
    "dfnorm\n",
    "\n",
    "dfs3 = pd.concat([dfs2,dfnorm])\n",
    "\n",
    "\n",
    "#check if what we have done is right\n",
    "dfs3[(dfs3['Turnstile_ID']=='A013-R081-01-06-01') & (dfs3['DATE']=='07/02/2021')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum all values that have more than 6 records \n",
    "dfs3['Num_Entries'] = pd.to_numeric(dfs3['Num_Entries'])\n",
    "dfs3['Num_Exits'] = pd.to_numeric(dfs3['Num_Exits'])\n",
    "\n",
    "##drop outliers again !\n",
    "dfs3 = dfs3[dfs3['Num_Entries']<5000]\n",
    "dfs3 = dfs3[dfs3['Num_Exits']<5000]\n",
    "\n",
    "\n",
    "four_hr_block =dfs3.groupby(['STATION','Turnstile_ID','DATE','Hours','DateHour']).sum().reset_index()\n",
    "four_hr_block.sort_values('Num_Entries',ascending=False)\n",
    "#print(four_hr_block.duplicated(['Turnstile_ID','DateHour']).sum())\n",
    "\n",
    "\n",
    "\n",
    "four_hr_block[four_hr_block['STATION']=='182-183 STS']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions to narrow down analysis\n",
    "## What are the stations that have the heaviest foot print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group by stations , calculate sum, and mean \n",
    "\n",
    "Total_Station = four_hr_block.groupby('STATION')[['Num_Entries','Num_Exits']].sum().reset_index()\n",
    "Total_Station.sort_values('Num_Entries',ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the peak time for each of the busiest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Station_34st_Turnstile_Daily = four_hr_block[four_hr_block['STATION']=='34 ST-HERALD SQ'].groupby(['DateHour','Turnstile_ID'])['Num_Entries'].sum().reset_index()\n",
    "Station_34st_Turnstile_Daily.columns =['DateHour','Turnstile_ID','Sum']\n",
    "#fig, ax = plt.subplots(figsize=(20,10)) \n",
    "#mask = grp.isnull()\n",
    "grp2 = Station_34st_Turnstile_Daily.pivot(index='Turnstile_ID',columns='DateHour',values='Sum')\n",
    "fig, ax = plt.subplots(figsize=(20,10)) \n",
    "ax = sns.heatmap(grp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Station_GRD_CNTRL_Daily = four_hr_block[four_hr_block['STATION']=='GRD CNTRL-42 ST'].groupby(['DATE','Turnstile_ID'])['Num_Entries'].sum().reset_index()\n",
    "Station_GRD_CNTRL_Daily.columns =['Date','Turnstile_ID','Sum']\n",
    "#fig, ax = plt.subplots(figsize=(20,10)) \n",
    "#mask = grp.isnull()\n",
    "grp2 = Station_GRD_CNTRL_Daily.pivot(index='Turnstile_ID',columns='Date',values='Sum')\n",
    "fig, ax = plt.subplots(figsize=(20,10)) \n",
    "ax = sns.heatmap(grp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "Station_TIMES_SQ_Daily = four_hr_block[four_hr_block['STATION']=='TIMES SQ-42 ST'].groupby(['DATE','Turnstile_ID'])['Num_Entries'].sum().reset_index()\n",
    "Station_TIMES_SQ_Daily.columns =['Date','Turnstile_ID','Sum']\n",
    "#fig, ax = plt.subplots(figsize=(20,10)) \n",
    "#mask = grp.isnull()\n",
    "grp2 = Station_TIMES_SQ_Daily.pivot(index='Turnstile_ID',columns='Date',values='Sum')\n",
    "fig, ax = plt.subplots(figsize=(20,10)) \n",
    "ax = sns.heatmap(grp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter and draw heat map for particular station across days. (All turnstiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Station_34st_Turnstile_Daily[Station_34st_Turnstile_Daily['Turnstile_ID']=='R143-R032-02-03-03'].hist()\n",
    "## checking for normal distribution to perform t-test\n",
    "\n",
    "Station_34st_Turnstile_Daily[Station_34st_Turnstile_Daily['Turnstile_ID']=='R147-R033-04-00-06'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Stations GPS Coordinates with Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting all files urls from MTA website (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import urllib.request as urllib2\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "from os.path import exists,getsize\n",
    "import os\n",
    "from socket import timeout\n",
    "from urllib.error import HTTPError, URLError\n",
    "\n",
    "retries = 0\n",
    "\n",
    "def download_all_files():\n",
    "    global retries\n",
    "    files_to_download = []\n",
    "    html_page = urllib2.urlopen(\"http://web.mta.info/developers/turnstile.html\")\n",
    "    soup = BeautifulSoup(html_page)\n",
    "    all_Data_Links = soup.findAll('a' ,attrs={'href': re.compile(\"^data\")})\n",
    "    for link in all_Data_Links:\n",
    "        #print(link['href'].split(\"/\"))\n",
    "        fname = link['href'].split(\"/\")[3]\n",
    "        #check if file already exists or less than 20 Mega in size(partial file)\n",
    "        path_to_file = \"D:\\\\Proj\\\\1\\\\\"+str(fname)\n",
    "        if not exists(path_to_file) or getsize(path_to_file) <20*10**6:\n",
    "            file = open(path_to_file,\"wb\")\n",
    "            try:\n",
    "                url = urllib2.urlopen('http://web.mta.info/developers/'+link['href'],timeout=120)\n",
    "                #reset retries upon sucessful download\n",
    "                print(fname,' Downloaded successfully')\n",
    "                retries =0\n",
    "            except (HTTPError, URLError) as error:\n",
    "                print('Data of %s not retrieved because %s\\nURL: %s', name, error, url)\n",
    "            except timeout:\n",
    "                print('socket timed out - URL %s', url)\n",
    "                \n",
    "                \n",
    "                retries +=1\n",
    "                if retries <4:\n",
    "                    download_all_files()\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Max retries reached\")\n",
    "                \n",
    "            for line in url:\n",
    "                #print(type(line))\n",
    "                file.write(line + b'\\n')\n",
    "            file.close()\n",
    "        #else:\n",
    "            #print(\"file \"+fname+\" skipped\")\n",
    "        # add each link to a list\n",
    "    #files_to_download.append('http://web.mta.info/developers/'+link['href'])\n",
    "\n",
    "\n",
    "download_all_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing to DB using a loop and threads (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pyodbc\n",
    "import time\n",
    "\n",
    "server = 'LAPTOP-88OFGUMN\\SQLEXPRESS' \n",
    "database = 'T5_Proj1' \n",
    "username = 'sa' \n",
    "password = '102030' \n",
    "#cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "#cursor = cnxn.cursor()\n",
    "\n",
    "onlyfiles = [f for f in listdir(\"D:\\\\Proj\\\\1\") if isfile(join(\"D:\\\\Proj\\\\1\", f))]\n",
    "def Import_CSV(files_path):\n",
    "    cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "    cursor = cnxn.cursor()\n",
    "\n",
    "    for afile in files_path:\n",
    "        start = time.time()\n",
    "        file_url = \"D:\\\\Proj\\\\1\\\\\"+afile\n",
    "        print(file_url)\n",
    "        df = pd.read_csv(file_url)\n",
    "        #trim white spaces in column header\n",
    "        df.columns=df.columns.str.strip()\n",
    "        i =0\n",
    "        for index, row in df.iterrows():\n",
    "            cursor.execute(\"Insert into MTA2 (C_A,Unit,SCP,Station,LineName,Division,Date_str,Time_str,Description,Entries,Exits) \\\n",
    "            values (?,?,?,?,?,?,?,?,?,?,?)\",(row['C/A'],row['UNIT'],row['SCP'],row['STATION'],row['LINENAME'],row['DIVISION'],row['DATE'],row['TIME'],row['DESC'],row['ENTRIES'],row['EXITS'] ))\n",
    "            cnxn.commit()\n",
    "            i  +=1\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        print(afile,\" has been added with \"+str(i)+ \" rows\")\n",
    "        print(\"took \"+str(elapsed)+ \" seconds to add records\")\n",
    "    cnxn.close()\n",
    "\n",
    "    \n",
    "## multi threading \n",
    "import _thread\n",
    "import time\n",
    "\n",
    "onlyfiles = [f for f in listdir(\"D:\\\\Proj\\\\1\") if isfile(join(\"D:\\\\Proj\\\\1\", f))]\n",
    "\n",
    "# Create two threads as follows\n",
    "#try:\n",
    "_thread.start_new_thread( Import_CSV, (onlyfiles[0:100] ,) )\n",
    "_thread.start_new_thread( Import_CSV, (onlyfiles[100:200],) )\n",
    "_thread.start_new_thread( Import_CSV, (onlyfiles[200:300],) )\n",
    "_thread.start_new_thread( Import_CSV, (onlyfiles[300:],) )\n",
    "#except:\n",
    " #  print(\"Error: unable to start thread\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
